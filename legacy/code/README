This repository contains the code for IJCAI submission 4750:
"A Markov Reward Process-Based Approach to Spatial Interpolation". Our code
is very much "research code"; though we tried to clean up our code by removing
unused functions and restructuring the files to be as accessible as possible, 
some of the functions are still based on legacy objectives, and would work 
differently had they been written later. Apart from cleaning up the code and 
adding some extra comments, we opted to provide our code as-is, for maximal 
reproducibility. Of course, if we were to release a Python package based on this 
thesis, we would take care to make our code more efficient, update legacy code, 
reduce the number of dependencies, and generally provide nicer code.

Dependencies

The code was written in Python 3.8.3, and has the following dependencies:
- Numpy             1.19.0
- Scipy             1.5.1
- Matplotlib        3.2.2
- Earthpy           0.9.2
- Pandas            1.0.5
- Geopandas         0.8.1
- Scikit-learn      0.22.2
- Auto-sklearn      0.8.0
- SMAC              0.12.2
- Tensorflow        2.3.0
- Auto-keras        1.0.8
- Rasterio          1.1.5
- Networkx          2.4
- Shapely           1.7.0
- Pykrige           1.5.0
- Cma               3.0.3
- Orange            3.26.0


How to use

The main files to run experiments can be found in the "run_files" directory. They
were used from the root directory; thus, in their current form, the desired run
script must be copied to the root directory first. All files are heavily reliant
on setting paths to the various data sources. The label set can be altered typically
below the assignment of bounding boxes and shapefiles to regions. Since there are
so many slightly different files, we mostly focussed on adding comments to "main_runs.py";
the other files will have nearly identical structure, so the comments will apply to
those as well. Using different files allowed us to run various different experiments
in parallel. In addition to "run_files", there are also three jupyter notebooks: one
for running data analysis (plots+significance testing) on the results, one to
perform NAS and save a tensorflow model for the CNNs, and one containing the code for
our synthetic data generation and the associated experiment. An HTML version of the notebooks
is also provided. Please consult "smac_configs.txt" for a summary of SMAC-configured
preprocessing configurations.


File organisation

The "preprocessing", "mrp" and "baselines" directories contain the core code
for the implementation of our methods and baselines, as well as data processing. 
All functions were annotated to hopefully give a reader an idea of what they do, and
how they correspond to certain elements of our paper. The "working" directory mainly
contains manually-defined taxonomy files, of which we only ended up using v1. The
"run_files" directory contains scripts we used to run our experiments on a computing
cluster. Within this directory, the "SMAC" subdirectory contains scripts for automated
algorithm configuration and preprocessing for those methods that needed it.


Data acquisition

Since the datasets we used are publicly available, we would rather link to the original
source than redistribute them ourselves. First off, we got our map data from OSM, distributed
by Geofabrik:
https://download.geofabrik.de/ (South Korea and Taiwan)
We only used a selection of the data in these zips; the names of the sets of files we
used were "gis_osm_buildings_a_free_1", "gis_osm_natural_free_1", "gis_osm_places_free_1", 
"gis_osm_pofw_free_1", "gis_osm_traffic_a_free_1" and "gis_osm_transport_a_free_1". 

For our ground truth data, we used World Bank GDP data that can be downloaded here:
https://datacatalog.worldbank.org/dataset/gross-domestic-product-2010
The Covid dataset was downloaded from:
https://dacon.io/competitions/official/235590/codeshare/1022 , but for those who wish to
avoid Korean language websites, it appears to also be hosted on Github here:
https://github.com/hwuiwon/covid-19-analysis-kr , though we cannot vouch for this version
since we downloaded it from Dacon. The only file in the dataset we used was PatientRoute.csv.



Notes

For legacy reasons, the methods for processing spatial feature data are a 
bit more elaborate than they need to be. We implemented our MRP methods using
the graph library networkx, but this was also for legacy reasons; there is no
need to do the same thing should anyone wish to re-implement our methods. In
reality every run file went through various iterations and slight variations
as we encountered (mostly computing cluster-related) problems; the files we
provided are the last versions we used. We also had several versions to run
different conditions, such as different training- and test sets, in parallel;
we only provided one file that can be changed as needed.

